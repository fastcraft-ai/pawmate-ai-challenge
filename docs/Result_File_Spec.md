# Result File Specification

## Purpose

This document defines the standardized format for benchmark result files that enable automated collection, validation, and aggregation of results from multiple developers and AI tools.

Result files are JSON-only for maximum automation and programmatic processing.

## File Format

Result files MUST be JSON (`.json`) files containing structured data.

### Structure

```json
{
  "schema_version": "1.0",
  "result_data": {
    "run_identity": {
      "tool_name": "string",
      "tool_version": "string",
      "run_id": "string",
      "run_number": 1,
      "target_model": "A",
      "api_style": "REST",
      "spec_reference": "string",
      "workspace_path": "string",
      "run_environment": "string"
    },
    "metrics": {
      "ttfr": {
        "start_timestamp": "ISO-8601 string",
        "end_timestamp": "ISO-8601 string",
        "minutes": "number or 'Unknown'"
      },
      "ttfc": {
        "start_timestamp": "ISO-8601 string",
        "end_timestamp": "ISO-8601 string",
        "minutes": "number or 'Unknown'"
      },
      "clarifications_count": "number or 'Unknown'",
      "interventions_count": "number or 'Unknown'",
      "reruns_count": "number or 'Unknown'",
      "acceptance": {
        "model": "A",
        "pass_count": "number or 'Unknown'",
        "fail_count": "number or 'Unknown'",
        "not_run_count": "number or 'Unknown'",
        "passrate": "number or 'Unknown'"
      },
      "determinism_compliance": "Pass|Fail|Unknown",
      "overreach_incidents_count": "number or 'Unknown'",
      "contract_completeness_passrate": "number or 'Unknown'",
      "instructions_quality_rating": "100|70|40|0|Unknown",
      "reproducibility_rating": "None|Minor|Major|Unknown"
    },
    "scores": {
      "correctness_C": "number or 'Unknown'",
      "reproducibility_R": "number or 'Unknown'",
      "determinism_D": "number or 'Unknown'",
      "effort_E": "number or 'Unknown'",
      "speed_S": "number or 'Unknown'",
      "contract_docs_K": "number or 'Unknown'",
      "penalty_overreach_PO": "number or 'Unknown'",
      "overall_score": "number or 'Unknown'"
    },
    "artifacts": {
      "tool_transcript_path": "string",
      "run_instructions_path": "string",
      "contract_artifact_path": "string",
      "acceptance_checklist_path": "string",
      "acceptance_evidence_path": "string",
      "determinism_evidence_path": "string",
      "overreach_evidence_path": "string",
      "ai_run_report_path": "string",
      "automated_tests_path": "string"
    },
    "submission": {
      "submitted_timestamp": "ISO-8601 string",
      "submitted_by": "string",
      "submission_method": "automated|manual"
    }
  }
}
```

## Naming Convention

Result files MUST follow this naming pattern:

```
{tool-slug}_{model}_{api-type}_{run-number}_{timestamp}.json
```

Where:
- `tool-slug`: Lowercase, alphanumeric + hyphens (e.g., `cursor-v0-43`, `github-copilot`)
- `model`: `modelA` or `modelB`
- `api-type`: `REST` or `GraphQL`
- `run-number`: `run1` or `run2`
- `timestamp`: ISO-8601 format without colons (e.g., `20241218T1430`)

Example: `cursor-v0-43_modelA_REST_run1_20241218T1430.json`

## Required Fields

### Schema Version
- **Field**: `schema_version`
- **Type**: String
- **Required**: Yes
- **Description**: Version of this specification (currently "1.0")
- **Purpose**: Enables future schema evolution

### Run Identity
All fields in `result_data.run_identity` are required and MUST match the values from `run.config` or be explicitly provided.

### Metrics
All fields in `result_data.metrics` are required. Use `"Unknown"` if evidence is missing (per `docs/Benchmarking_Method.md` evidence-first rule).

### Scores
All fields in `result_data.scores` are required. May be `"Unknown"` if required metrics are unknown.

### Artifacts
All fields in `result_data.artifacts` are required. Paths MUST be relative to the repository root or absolute paths. Use empty string `""` if artifact is missing.

### Submission Metadata
- **submitted_timestamp**: ISO-8601 UTC timestamp when result was generated/submitted
- **submitted_by**: Identifier of submitter (e.g., GitHub username, email, or "AI-tool-name")
- **submission_method**: `"automated"` if generated by AI tool, `"manual"` if created by operator

## Notes

Result files are JSON-only for maximum automation. For human-readable reports, see the generated comparison reports in `results/compiled/`.

## Validation Rules

### Format Validation
1. File MUST be valid JSON
2. File MUST validate against JSON schema (`results/schemas/result-schema-v1.0.json`)
3. All required fields MUST be present
4. Field types MUST match specification
5. Enum values MUST be from allowed sets
6. Timestamps MUST be valid ISO-8601 format
7. File name MUST match naming convention

### Data Validation
1. `run_number` MUST be 1 or 2
2. `target_model` MUST be "A" or "B"
3. `api_style` MUST be "REST" or "GraphQL"
4. `acceptance.model` MUST match `target_model`
5. Numeric scores MUST be in range 0-100 (or "Unknown")
6. Pass rates MUST be in range 0-1 (or "Unknown")
7. Timestamps MUST be chronologically valid (end >= start)

### Evidence Validation
1. Referenced artifact paths SHOULD exist (warning if missing, not error)
2. Evidence paths SHOULD be accessible (warning if not, not error)

## Schema Evolution

Future schema versions will:
- Maintain backward compatibility where possible
- Use semantic versioning (MAJOR.MINOR.PATCH)
- Document migration paths for breaking changes
- Support multiple schema versions during transition periods

## Example

See `results/result_template.json` for a complete example with all required fields populated.

## Related Documents

- `docs/Benchmarking_Method.md` - Metric definitions and evidence requirements
- `docs/Scoring_Rubric.md` - Scoring calculations
- `docs/Comparison_Report_Template.md` - Aggregation output format

